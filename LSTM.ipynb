{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNDNPs3WCIUcyr+N2rPj5y0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ofaKWGAlHJdZ","colab_type":"code","outputId":"f95a766e-61db-45d3-b472-867c704d109a","executionInfo":{"status":"ok","timestamp":1586731950275,"user_tz":420,"elapsed":103198,"user":{"displayName":"Joshua Kim","photoUrl":"","userId":"06841281877880938662"}},"colab":{"base_uri":"https://localhost:8080/","height":215}},"source":["# import libraries \n","\n","import pandas as pd\n","import numpy as np\n","from tensorflow.python.keras.models import Sequential\n","from tensorflow.python.keras.layers import Dense\n","from tensorflow.python.keras.layers import LSTM\n","from tensorflow.python.keras.layers import Dropout\n","from tensorflow.python.keras import optimizers\n","from sklearn.model_selection import train_test_split\n","import tensorflow.python.keras.layers\n","import tensorflow as tf\n","from google.colab import files\n","from tensorflow.python.keras.callbacks import EarlyStopping\n","from tensorflow.python.keras.callbacks import ModelCheckpoint\n","\n","# read the data file\n","# I ran the same block multiple times, just changing the name of the csv files to iterate through period 1~6\n","df = pd.read_csv('returns_6.csv',header=None)\n","data = df.to_numpy()\n","data = np.array(data).ravel()\n","# convert data to array\n","\n","# window length was chosen as 20, which is noted as b_size\n","b_size = 20\n","\n","# this function changes the shape of the data as LSTM requires\n","def build_timeseries(array):\n","    dim_0 = array.shape[0] - b_size\n","    x = np.zeros((dim_0, b_size))\n","    y = np.zeros((dim_0,))\n","    \n","    for i in range(dim_0):\n","        x[i] = array[i:b_size+i]\n","        y[i] = array[b_size+i]\n","\n","    # dimension is expanded, as required for LSTM inputs\n","    x = np.expand_dims(x, axis=2)\n","    print(\"length of time-series i/o\",x.shape,y.shape)\n","    return x, y\n","x,y = build_timeseries(data)\n","\n","# the following lines split the data into 70% training, 20% validation, and 10% test sets\n","x_train, x_test = train_test_split(x, train_size=0.7, test_size=0.3, shuffle=False)\n","x_valid, x_test = train_test_split(x_test, train_size=2/3, test_size=1/3, shuffle=False)\n","y_train, y_test = train_test_split(y, train_size=0.7, test_size=0.3, shuffle=False)\n","y_valid, y_test = train_test_split(y_test, train_size=2/3, test_size=1/3, shuffle=False)\n","\n","# convert the outputs to 0 or 1, depending on whether it is less than 0 or not\n","# 1 indicates market going up or staying the same and 0 indicates market going down\n","y_train=np.where(y_train>=0,1,0)\n","y_test=np.where(y_test>=0,1,0)\n","y_valid=np.where(y_valid>=0,1,0)\n","\n","# early stopping was employed\n","es = EarlyStopping(monitor='val_loss', mode='min',patience=0)\n","\n","# lstm model is implemented here\n","lstm_model = Sequential()\n","# the first layer. return_sequences should be False if this is the only LSTM layer being used\n","lstm_model.add(LSTM(25, batch_input_shape=(1, b_size,1), dropout=0.1,stateful=True,return_sequences=True))\n","# the second layer, was not used for the original LSTM model\n","lstm_model.add(LSTM(50,  dropout=0.1,stateful=True,return_sequences=True))\n","# the third layer, was not used for the original LSTM model\n","lstm_model.add(LSTM(25,  dropout=0.1,stateful=True))\n","# the dense layer which serves as output\n","lstm_model.add(Dense(1,activation='sigmoid'))\n","\n","# the model is trained with binary crossentropy as the loss function, adam optimizer, and accuracy metrics\n","lstm_model.compile(loss='binary_crossentropy',\n","              optimizer = 'adam',\n","              metrics =['accuracy'])\n","# the input was scaled by 20 for better convergence as input values were too small to be used without being scaeld\n","# the input will always be scaled by 20\n","# the model stucture is explained in the paper\n","lstm_model.fit(x_train*20, y_train,validation_data=(x_valid*20,y_valid), epochs=1000, \n","               batch_size=1, verbose=1, shuffle=False, callbacks=[es])"],"execution_count":19,"outputs":[{"output_type":"stream","text":["length of time-series i/o (3755, 20, 1) (3755,)\n","Epoch 1/1000\n","2628/2628 [==============================] - 25s 10ms/step - loss: 0.2485 - accuracy: 0.5495 - val_loss: 0.2487 - val_accuracy: 0.5379\n","Epoch 2/1000\n","2628/2628 [==============================] - 24s 9ms/step - loss: 0.2481 - accuracy: 0.5491 - val_loss: 0.2486 - val_accuracy: 0.5379\n","Epoch 3/1000\n","2628/2628 [==============================] - 24s 9ms/step - loss: 0.2480 - accuracy: 0.5495 - val_loss: 0.2486 - val_accuracy: 0.5379\n","Epoch 4/1000\n","2628/2628 [==============================] - 24s 9ms/step - loss: 0.2479 - accuracy: 0.5495 - val_loss: 0.2486 - val_accuracy: 0.5379\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f51675046d8>"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"Zfj2K6rJVH_P","colab_type":"code","outputId":"fe326d45-942b-43ee-a814-2872ce279700","executionInfo":{"status":"ok","timestamp":1586731968529,"user_tz":420,"elapsed":2244,"user":{"displayName":"Joshua Kim","photoUrl":"","userId":"06841281877880938662"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# quickly evaluate model performnace\n","lstm_model.evaluate(x_test*20,y_test,batch_size=1)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["376/376 [==============================] - 2s 4ms/step - loss: 0.2472 - accuracy: 0.5585\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.24716107547283173, 0.5585106611251831]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"zyAtlwG8y4yg","colab_type":"code","outputId":"e3a65fc5-1b8e-46c4-b0ee-872e1d825643","executionInfo":{"status":"ok","timestamp":1586731973454,"user_tz":420,"elapsed":3447,"user":{"displayName":"Joshua Kim","photoUrl":"","userId":"06841281877880938662"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# make predicition with the current LSTM model and save the result as csv files\n","# Just like reading the data file at the beginning, I simply change the output file name for each iteration\n","pred = lstm_model.predict(x_test*20,batch_size=1)\n","pred = pred.flatten()\n","np.savetxt(\"n_result6.csv\", pred, delimiter=\",\")\n","files.download('n_result6.csv')\n","pred"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.5343074 , 0.5342568 , 0.5342055 , 0.5341652 , 0.5341751 ,\n","       0.53426015, 0.5342479 , 0.53426313, 0.53429383, 0.5342912 ,\n","       0.5342932 , 0.534331  , 0.53435105, 0.5343428 , 0.5343138 ,\n","       0.5342568 , 0.5342855 , 0.53434443, 0.5343923 , 0.53435946,\n","       0.5343511 , 0.5343105 , 0.5342757 , 0.53425807, 0.5342553 ,\n","       0.53427815, 0.5343055 , 0.5343622 , 0.534392  , 0.53434336,\n","       0.5343709 , 0.5343163 , 0.5342845 , 0.5342707 , 0.534268  ,\n","       0.5342837 , 0.53431004, 0.53428453, 0.5342443 , 0.53425545,\n","       0.5342453 , 0.53429455, 0.5343177 , 0.5343398 , 0.5343615 ,\n","       0.5343819 , 0.5343871 , 0.5343623 , 0.5343237 , 0.53431004,\n","       0.5342789 , 0.5342798 , 0.5343242 , 0.53430223, 0.53429526,\n","       0.53425306, 0.53426546, 0.5343052 , 0.53432906, 0.53435546,\n","       0.53433716, 0.53433096, 0.53430533, 0.5343071 , 0.5343084 ,\n","       0.53436184, 0.5343973 , 0.5343877 , 0.53437364, 0.5345312 ,\n","       0.53462285, 0.53447473, 0.53441805, 0.5342247 , 0.5341894 ,\n","       0.5342824 , 0.5343061 , 0.534331  , 0.534355  , 0.5345088 ,\n","       0.53437316, 0.5344112 , 0.53441703, 0.534282  , 0.5341759 ,\n","       0.53410923, 0.53417754, 0.53419095, 0.53418803, 0.5340924 ,\n","       0.53415203, 0.5342679 , 0.53443235, 0.53445613, 0.5344764 ,\n","       0.53437924, 0.5343261 , 0.53440607, 0.53450435, 0.5344644 ,\n","       0.5344434 , 0.5343033 , 0.53424424, 0.53409106, 0.53412294,\n","       0.5341276 , 0.5341159 , 0.53435016, 0.534412  , 0.5345342 ,\n","       0.5344991 , 0.5344436 , 0.53436035, 0.53432274, 0.53441495,\n","       0.5345189 , 0.5344838 , 0.5345086 , 0.5345394 , 0.5345844 ,\n","       0.5346535 , 0.53424054, 0.53407866, 0.5340762 , 0.5340675 ,\n","       0.53410715, 0.5342977 , 0.5341272 , 0.53408796, 0.534078  ,\n","       0.5341165 , 0.53415924, 0.5342228 , 0.5343074 , 0.5342813 ,\n","       0.53428847, 0.53426427, 0.5342088 , 0.53432614, 0.53435695,\n","       0.5343661 , 0.5343064 , 0.53435445, 0.5343754 , 0.5342771 ,\n","       0.53422046, 0.53424007, 0.5342269 , 0.5342316 , 0.53427947,\n","       0.5343676 , 0.5343824 , 0.53437483, 0.5342868 , 0.53426117,\n","       0.5342932 , 0.53424746, 0.5342535 , 0.5342725 , 0.53431875,\n","       0.53429985, 0.53430074, 0.53432024, 0.53433603, 0.5343571 ,\n","       0.5343159 , 0.53433603, 0.53434634, 0.5343839 , 0.5344265 ,\n","       0.53442377, 0.5343037 , 0.5342589 , 0.53421694, 0.5342399 ,\n","       0.5342379 , 0.53423786, 0.5342642 , 0.5343072 , 0.53426015,\n","       0.5343782 , 0.5344005 , 0.53434604, 0.5343551 , 0.5343272 ,\n","       0.5342775 , 0.5342071 , 0.5342226 , 0.5342438 , 0.5342636 ,\n","       0.5342617 , 0.53427505, 0.5343336 , 0.5343294 , 0.53433335,\n","       0.5342954 , 0.53430223, 0.53431123, 0.53433573, 0.5343312 ,\n","       0.5343294 , 0.5342757 , 0.5342909 , 0.5343096 , 0.5342927 ,\n","       0.5342932 , 0.53429735, 0.5343536 , 0.5343745 , 0.53430945,\n","       0.53432477, 0.53442264, 0.53443116, 0.53442013, 0.5343647 ,\n","       0.5344679 , 0.53440017, 0.53431946, 0.5342351 , 0.53426266,\n","       0.53431654, 0.5342718 , 0.53428584, 0.5343613 , 0.5343554 ,\n","       0.53438866, 0.53441226, 0.53437567, 0.5344198 , 0.5344096 ,\n","       0.5342412 , 0.5341634 , 0.5341393 , 0.5341132 , 0.53413326,\n","       0.5341974 , 0.5342614 , 0.53427494, 0.5343072 , 0.5343223 ,\n","       0.53427535, 0.5342653 , 0.53422886, 0.5342649 , 0.5343087 ,\n","       0.53439075, 0.53440976, 0.5343752 , 0.53432536, 0.5342698 ,\n","       0.53425354, 0.53422564, 0.5342615 , 0.5343199 , 0.53433377,\n","       0.5343132 , 0.5343025 , 0.5342815 , 0.53429246, 0.53432876,\n","       0.53437823, 0.5343584 , 0.5343821 , 0.53435665, 0.53430086,\n","       0.5342661 , 0.53430426, 0.5342737 , 0.53429043, 0.53431845,\n","       0.5343911 , 0.53444237, 0.5344653 , 0.53459185, 0.5344685 ,\n","       0.53438383, 0.53421456, 0.53422856, 0.53431636, 0.5342379 ,\n","       0.5343963 , 0.5343901 , 0.5342746 , 0.5341724 , 0.5342244 ,\n","       0.53420585, 0.53423417, 0.53440547, 0.5343585 , 0.534355  ,\n","       0.5343021 , 0.53421384, 0.53421646, 0.53429013, 0.53424823,\n","       0.5341809 , 0.5341975 , 0.53424275, 0.53428257, 0.534264  ,\n","       0.53427225, 0.5342995 , 0.5343383 , 0.5343358 , 0.53433985,\n","       0.5343413 , 0.5343723 , 0.53437656, 0.5344135 , 0.5343643 ,\n","       0.5343554 , 0.5343725 , 0.53433186, 0.5343894 , 0.53448045,\n","       0.53440726, 0.534272  , 0.5342696 , 0.5343635 , 0.53431267,\n","       0.5342569 , 0.5341853 , 0.53420836, 0.53417933, 0.53422344,\n","       0.5342452 , 0.53429925, 0.5342806 , 0.5343153 , 0.53431153,\n","       0.5343082 , 0.5342922 , 0.5342667 , 0.5342884 , 0.53429145,\n","       0.5343233 , 0.53427553, 0.5342622 , 0.5342907 , 0.5343066 ,\n","       0.53430486, 0.5342989 , 0.5343225 , 0.534323  , 0.53432536,\n","       0.5343245 , 0.5342822 , 0.5342833 , 0.5343029 , 0.53433967,\n","       0.5343578 , 0.5343429 , 0.5342899 , 0.5342753 , 0.53426087,\n","       0.53430223, 0.53436995, 0.5344157 , 0.53436613, 0.53433317,\n","       0.5342644 , 0.53428155, 0.5343032 , 0.53429663, 0.53425014,\n","       0.534259  , 0.53423494, 0.5342548 , 0.5342846 , 0.5342809 ,\n","       0.5342672 , 0.5342792 , 0.5343003 , 0.53428733, 0.5343    ,\n","       0.5343508 ], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":21}]}]}